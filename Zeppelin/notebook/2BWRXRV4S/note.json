{"paragraphs":[{"text":"%dep\nz.load(\"com.databricks:spark-csv_2.10:1.5.0\")","dateUpdated":"2016-09-26T15:25:08+0000","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1474903371634_632780767","id":"20160926-152251_145531742","result":{"code":"SUCCESS","type":"TEXT","msg":"DepInterpreter(%dep) deprecated. Load dependency through GUI interpreter menu instead.\nres0: org.apache.zeppelin.dep.Dependency = org.apache.zeppelin.dep.Dependency@d36e788\n"},"dateCreated":"2016-09-26T15:22:51+0000","dateStarted":"2016-09-26T15:25:08+0000","dateFinished":"2016-09-26T15:25:16+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:3713"},{"text":"import org.apache.spark.mllib.classification.{LogisticRegressionModel, LogisticRegressionWithLBFGS}\nimport org.apache.spark.mllib.evaluation.BinaryClassificationMetrics\nimport org.apache.spark.mllib.regression.LabeledPoint\nimport org.apache.spark.mllib.linalg.{DenseVector, SparseVector}\nimport org.apache.spark.sql.SQLContext\nimport org.apache.spark.ml.feature.{HashingTF, IDF, Tokenizer}\n\nval sentenceData = sqlContext.createDataFrame(Seq(\n  (1.0, \"has the best\"),\n  (1.0, \"Always seem to find the best\"),\n  (1.0, \"I am really happy with my new\"),\n  (1.0, \"I can always count on\"),\n  (1.0, \"for awesome\"),\n  (1.0, \"always has the best prices on\"),\n  (0.0, \"has the worst\"),\n  (0.0, \"always has the best prices on\"),\n  (0.0, \"I can never find any\"),\n  (0.0, \"that I am looking for at\"),\n  (0.0, \"I absolutely hate the I just got from\"),\n  (0.0, \"for disappointing\"),\n  (0.0, \"trying to rip me off when I shop for\")\n)).toDF(\"label\", \"sentence\")\n\nval tokenizer = new Tokenizer().setInputCol(\"sentence\").setOutputCol(\"words\")\nval wordsData = tokenizer.transform(sentenceData)\nval hashingTF = new HashingTF().setInputCol(\"words\").setOutputCol(\"rawFeatures\").setNumFeatures(20)\nval featurizedData = hashingTF.transform(wordsData)\n\nfeaturizedData.show\n\nval training = featurizedData.rdd.map(row => LabeledPoint(row.getAs[Double](\"label\"), row.getAs[DenseVector](\"rawFeatures\")))\n\nval model = new LogisticRegressionWithLBFGS()\n  .setNumClasses(10)\n  .run(training)\n  \nval testData = sqlContext.createDataFrame(Seq((1.0,\"has the best\"))).toDF(\"label\",\"sentence\")\n\nval tokenizerTest = new Tokenizer().setInputCol(\"sentence\").setOutputCol(\"words\")\nval wordsDataTest = tokenizerTest.transform(testData)\nval hashingTFTest = new HashingTF().setInputCol(\"words\").setOutputCol(\"rawFeatures\").setNumFeatures(20)\nval featurizedDataTest = hashingTFTest.transform(wordsDataTest)\n\nfeaturizedDataTest.show\n\nval test = featurizedDataTest.rdd.map(row => (row.getAs[SparseVector](\"rawFeatures\")))\n\ntest.map { x => model.predict(x)}.collect.foreach(println)\n","dateUpdated":"2016-09-26T15:25:22+0000","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1474903393957_-1054312512","id":"20160926-152313_823796487","result":{"code":"SUCCESS","type":"TEXT","msg":"import org.apache.spark.mllib.classification.{LogisticRegressionModel, LogisticRegressionWithLBFGS}\nimport org.apache.spark.mllib.evaluation.BinaryClassificationMetrics\nimport org.apache.spark.mllib.regression.LabeledPoint\nimport org.apache.spark.mllib.linalg.{DenseVector, SparseVector}\nimport org.apache.spark.sql.SQLContext\nimport org.apache.spark.ml.feature.{HashingTF, IDF, Tokenizer}\nsentenceData: org.apache.spark.sql.DataFrame = [label: double, sentence: string]\ntokenizer: org.apache.spark.ml.feature.Tokenizer = tok_8a7799391494\nwordsData: org.apache.spark.sql.DataFrame = [label: double, sentence: string, words: array<string>]\nhashingTF: org.apache.spark.ml.feature.HashingTF = hashingTF_959152f99d9c\nfeaturizedData: org.apache.spark.sql.DataFrame = [label: double, sentence: string, words: array<string>, rawFeatures: vector]\n+-----+--------------------+--------------------+--------------------+\n|label|            sentence|               words|         rawFeatures|\n+-----+--------------------+--------------------+--------------------+\n|  1.0|        has the best|    [has, the, best]|(20,[0,1,6],[1.0,...|\n|  1.0|Always seem to fi...|[always, seem, to...|(20,[0,1,6,7,11,1...|\n|  1.0|I am really happy...|[i, am, really, h...|(20,[0,3,5,14,16]...|\n|  1.0|I can always coun...|[i, can, always, ...|(20,[3,5,11,16],[...|\n|  1.0|         for awesome|      [for, awesome]|(20,[7,17],[1.0,1...|\n|  1.0|always has the be...|[always, has, the...|(20,[0,1,6,10,11]...|\n|  0.0|       has the worst|   [has, the, worst]|(20,[1,6,15],[1.0...|\n|  0.0|always has the be...|[always, has, the...|(20,[0,1,6,10,11]...|\n|  0.0|I can never find any|[i, can, never, f...|(20,[4,5,8,16,17]...|\n|  0.0|that I am looking...|[that, i, am, loo...|(20,[3,5,7,16,17]...|\n|  0.0|I absolutely hate...|[i, absolutely, h...|(20,[0,1,4,5,6,8,...|\n|  0.0|   for disappointing|[for, disappointing]|(20,[3,17],[1.0,1...|\n|  0.0|trying to rip me ...|[trying, to, rip,...|(20,[0,1,2,3,5,7,...|\n+-----+--------------------+--------------------+--------------------+\n\ntraining: org.apache.spark.rdd.RDD[org.apache.spark.mllib.regression.LabeledPoint] = MapPartitionsRDD[2] at map at <console>:43\nmodel: org.apache.spark.mllib.classification.LogisticRegressionModel = org.apache.spark.mllib.classification.LogisticRegressionModel: intercept = 0.0, numFeatures = 180, numClasses = 10, threshold = 0.5\ntestData: org.apache.spark.sql.DataFrame = [label: double, sentence: string]\ntokenizerTest: org.apache.spark.ml.feature.Tokenizer = tok_1100ab3e3917\nwordsDataTest: org.apache.spark.sql.DataFrame = [label: double, sentence: string, words: array<string>]\nhashingTFTest: org.apache.spark.ml.feature.HashingTF = hashingTF_2f116d4f6f20\nfeaturizedDataTest: org.apache.spark.sql.DataFrame = [label: double, sentence: string, words: array<string>, rawFeatures: vector]\n+-----+------------+----------------+--------------------+\n|label|    sentence|           words|         rawFeatures|\n+-----+------------+----------------+--------------------+\n|  1.0|has the best|[has, the, best]|(20,[0,1,6],[1.0,...|\n+-----+------------+----------------+--------------------+\n\ntest: org.apache.spark.rdd.RDD[org.apache.spark.mllib.linalg.SparseVector] = MapPartitionsRDD[37] at map at <console>:43\n1.0\n"},"dateCreated":"2016-09-26T15:23:13+0000","dateStarted":"2016-09-26T15:25:22+0000","dateFinished":"2016-09-26T15:26:00+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:3714"},{"config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1474903522272_-329309311","id":"20160926-152522_480330757","dateCreated":"2016-09-26T15:25:22+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:3715","text":"println(model.intercept)\nprintln(model.weights)","dateUpdated":"2016-09-26T22:18:08+0000","dateFinished":"2016-09-26T22:18:08+0000","dateStarted":"2016-09-26T22:18:08+0000","result":{"code":"SUCCESS","type":"TEXT","msg":"0.0\n[9.660105997175553,-0.9543439229635031,-21.95262231731812,-17.127027183848586,-15.734763984748566,-3.387759505116247,5.317833881983485,14.210394214884621,-15.734763984748566,0.0,-37.35054842080012,11.663727208235112,0.0,0.0,-0.966296142871593,-20.76068195712328,10.026128658930098,-0.7597838637972363,-16.103654890201277,0.0,-3.151735282431342,-7.032402480475741,-1.8434159245356294,-5.762432347094956,-4.530030325794325,-3.8807517466956476,-6.505712216322704,-4.785091248287144,-4.530030325794325,0.0,-2.426570034627619,-1.949345359706894,0.0,0.0,-1.756324702146554,-6.4774396063424335,-3.005970746874371,-10.953937451640666,-2.495592114165221,0.0,-3.151735282431342,-7.032402480475741,-1.8434159245356294,-5.762432347094956,-4.530030325794325,-3.8807517466956476,-6.505712216322704,-4.785091248287144,-4.530030325794325,0.0,-2.426570034627619,-1.949345359706894,0.0,0.0,-1.756324702146554,-6.4774396063424335,-3.005970746874371,-10.953937451640666,-2.495592114165221,0.0,-3.151735282431342,-7.032402480475741,-1.8434159245356294,-5.762432347094956,-4.530030325794325,-3.8807517466956476,-6.505712216322704,-4.785091248287144,-4.530030325794325,0.0,-2.426570034627619,-1.949345359706894,0.0,0.0,-1.756324702146554,-6.4774396063424335,-3.005970746874371,-10.953937451640666,-2.495592114165221,0.0,-3.151735282431342,-7.032402480475741,-1.8434159245356294,-5.762432347094956,-4.530030325794325,-3.8807517466956476,-6.505712216322704,-4.785091248287144,-4.530030325794325,0.0,-2.426570034627619,-1.949345359706894,0.0,0.0,-1.756324702146554,-6.4774396063424335,-3.005970746874371,-10.953937451640666,-2.495592114165221,0.0,-3.151735282431342,-7.032402480475741,-1.8434159245356294,-5.762432347094956,-4.530030325794325,-3.8807517466956476,-6.505712216322704,-4.785091248287144,-4.530030325794325,0.0,-2.426570034627619,-1.949345359706894,0.0,0.0,-1.756324702146554,-6.4774396063424335,-3.005970746874371,-10.953937451640666,-2.495592114165221,0.0,-3.151735282431342,-7.032402480475741,-1.8434159245356294,-5.762432347094956,-4.530030325794325,-3.8807517466956476,-6.505712216322704,-4.785091248287144,-4.530030325794325,0.0,-2.426570034627619,-1.949345359706894,0.0,0.0,-1.756324702146554,-6.4774396063424335,-3.005970746874371,-10.953937451640666,-2.495592114165221,0.0,-3.151735282431342,-7.032402480475741,-1.8434159245356294,-5.762432347094956,-4.530030325794325,-3.8807517466956476,-6.505712216322704,-4.785091248287144,-4.530030325794325,0.0,-2.426570034627619,-1.949345359706894,0.0,0.0,-1.756324702146554,-6.4774396063424335,-3.005970746874371,-10.953937451640666,-2.495592114165221,0.0,-3.151735282431342,-7.032402480475741,-1.8434159245356294,-5.762432347094956,-4.530030325794325,-3.8807517466956476,-6.505712216322704,-4.785091248287144,-4.530030325794325,0.0,-2.426570034627619,-1.949345359706894,0.0,0.0,-1.756324702146554,-6.4774396063424335,-3.005970746874371,-10.953937451640666,-2.495592114165221,0.0]\n"},"focus":true},{"config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1474904402402_-153972203","id":"20160926-154002_47607153","dateCreated":"2016-09-26T15:40:02+0000","status":"READY","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:3924"}],"name":"Sentiment Analysis","id":"2BWRXRV4S","angularObjects":{"2BXZ2A649:shared_process":[],"2BVMEATMN:shared_process":[],"2BXRYURKA:shared_process":[],"2BY1WZT2D:shared_process":[],"2BW2CR9SY:shared_process":[],"2BYATHBKJ:shared_process":[]},"config":{"looknfeel":"default"},"info":{}}